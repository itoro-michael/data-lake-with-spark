{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8875da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44cf7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, dayofweek, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a56b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427c4559",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2a5c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "input_data = \"data/\"\n",
    "\n",
    "# get filepath to song data file\n",
    "song_data = input_data + \"song_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "042b8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_filename = glob.glob('data/song_data/**/*.json', recursive=True)\n",
    "\n",
    "# read song data file\n",
    "df = spark.read.json(list_filename)\n",
    "\n",
    "pd_df = df.toPandas() # Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7fb3454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_latitude</th>\n",
       "      <th>artist_location</th>\n",
       "      <th>artist_longitude</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>duration</th>\n",
       "      <th>num_songs</th>\n",
       "      <th>song_id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARDR4AC1187FB371A1</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montserrat Caballé;Placido Domingo;Vicente Sar...</td>\n",
       "      <td>511.16363</td>\n",
       "      <td>1</td>\n",
       "      <td>SOBAYLL12A8C138AF9</td>\n",
       "      <td>Sono andati? Fingevo di dormire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            artist_id  artist_latitude artist_location  artist_longitude  \\\n",
       "0  ARDR4AC1187FB371A1              NaN                               NaN   \n",
       "\n",
       "                                         artist_name   duration  num_songs  \\\n",
       "0  Montserrat Caballé;Placido Domingo;Vicente Sar...  511.16363          1   \n",
       "\n",
       "              song_id                            title  year  \n",
       "0  SOBAYLL12A8C138AF9  Sono andati? Fingevo di dormire     0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c705db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['artist_id', 'artist_latitude', 'artist_location', 'artist_longitude',\n",
       "       'artist_name', 'duration', 'num_songs', 'song_id', 'title', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4439b69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f090a87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f3ee94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = df.select([\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ab99ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log data file\n",
    "df = spark.read.json(\"./data/log_data\")\n",
    "\n",
    "pd_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3678885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harmonia</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>Smith</td>\n",
       "      <td>655.77751</td>\n",
       "      <td>free</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.541017e+12</td>\n",
       "      <td>583</td>\n",
       "      <td>Sehr kosmisch</td>\n",
       "      <td>200</td>\n",
       "      <td>1542241826796</td>\n",
       "      <td>\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     artist       auth firstName gender  itemInSession lastName     length  \\\n",
       "0  Harmonia  Logged In      Ryan      M              0    Smith  655.77751   \n",
       "\n",
       "  level                            location method      page  registration  \\\n",
       "0  free  San Jose-Sunnyvale-Santa Clara, CA    PUT  NextSong  1.541017e+12   \n",
       "\n",
       "   sessionId           song  status             ts  \\\n",
       "0        583  Sehr kosmisch     200  1542241826796   \n",
       "\n",
       "                                           userAgent userId  \n",
       "0  \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...     26  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23828e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['artist', 'auth', 'firstName', 'gender', 'itemInSession', 'lastName',\n",
       "       'length', 'level', 'location', 'method', 'page', 'registration',\n",
       "       'sessionId', 'song', 'status', 'ts', 'userAgent', 'userId'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4e32520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8056, 18)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ad1dbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c29434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = configparser.ConfigParser()\n",
    "# config.read('dl.cfg')\n",
    "\n",
    "# os.environ['AWS_ACCESS_KEY_ID']=config['AWS_ACCESS_KEY_ID']\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\" Creates spark session object.\n",
    "    Parameters:\n",
    "        None\n",
    "    Returns:\n",
    "        SparkSession: Spark session object.\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "\n",
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\" Reads JSON files from S3, creates dimension tables from the files, \n",
    "    and writes the tables back to S3 as parquet files.\n",
    "    Parameters:\n",
    "        spark (SparkSession): A spark session object.\n",
    "        input_data (str): The S3 bucket url for data input.\n",
    "        output_data (str): The S3 bucket url for data output.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + \"song_data/*/*/*/*.json\"\n",
    "    \n",
    "    # read song data file from s3\n",
    "    df = spark.read.json(song_data)\n",
    "    df.persist()\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select([\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"]) \\\n",
    "                    .dropDuplicates()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.mode(\"overwrite\").partitionBy('year', 'artist_id') \\\n",
    "               .parquet(output_data + 'songs')\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select([\"artist_id\", \"artist_name\", \"artist_location\", \n",
    "                               \"artist_latitude\", \"artist_longitude\"]) \\\n",
    "                      .dropDuplicates() \\\n",
    "                      .withColumnRenamed(\"artist_name\", \"name\") \\\n",
    "                      .withColumnRenamed(\"artist_location\", \"location\") \\\n",
    "                      .withColumnRenamed(\"artist_latitude\", \"latitude\") \\\n",
    "                      .withColumnRenamed(\"artist_longitude\", \"longitude\")\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.mode(\"overwrite\").parquet(os.path.join(output_data + 'artists'))\n",
    "\n",
    "\n",
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\" Reads JSON files from S3, creates dimension and fact tables from the files, \n",
    "    and writes the tables back to S3 as parquet files.\n",
    "    Parameters:\n",
    "        spark (SparkSession): A spark session object.\n",
    "        input_data (str): The S3 bucket url for data input.\n",
    "        output_data (str): The S3 bucket url for data output.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + \"log_data/*/*/*.json\"\n",
    "    \n",
    "    # get filepath to song data file\n",
    "    song_data = output_data + \"songs\"\n",
    "    \n",
    "    # get filepath to artist data file\n",
    "    artist_data = output_data + \"artists\"\n",
    "\n",
    "    # read log data file from s3\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.select(df.columns).where(df.page == \"NextSong\") \\\n",
    "           .dropDuplicates()\n",
    "    \n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select([\"userId\", \"firstName\", \"lastName\", \"gender\", \"level\"])\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode(\"overwrite\").parquet(output_data + 'users')\n",
    "\n",
    "    # create timestamp column from original timestamp column    \n",
    "    get_timestamp = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0), TimestampType())\n",
    "    df = df.withColumn(\"timestamp\", get_timestamp(df.ts))\n",
    "    \n",
    "    # create datetime column from original timestamp column    \n",
    "    get_datetime = udf(lambda x: x)\n",
    "    df = df.withColumn(\"start_time\", get_datetime(df.timestamp))\n",
    "    \n",
    "    df = df.withColumn(\"hour\", hour(\"timestamp\"))\n",
    "    \n",
    "    df = df.withColumn(\"day\", dayofmonth(\"timestamp\"))\n",
    "    \n",
    "    df = df.withColumn(\"week\",  weekofyear(\"timestamp\"))\n",
    "    \n",
    "    df = df.withColumn(\"month\", month(\"timestamp\"))\n",
    "    \n",
    "    df = df.withColumn(\"year\", year(\"timestamp\"))\n",
    "    \n",
    "    df = df.withColumn(\"weekday\", dayofweek(\"timestamp\"))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.select([\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\"])\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.mode(\"overwrite\").partitionBy('year', 'month') \\\n",
    "              .parquet(output_data + 'time_table')\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.parquet(song_data)\n",
    "    \n",
    "    # read in artists data to use for songplays table\n",
    "    artist_df = spark.read.parquet(artist_data).withColumnRenamed(\"artist_id\", \"id\")\n",
    "    \n",
    "    # Join song and artist table\n",
    "    song_artist_df = song_df.join(artist_df, song_df.artist_id == artist_df.id, how='inner')\n",
    "\n",
    "    # extract columns from joined song_artist and log datasets to create songplays table \n",
    "    songplays_table = df.join(song_artist_df, \\\n",
    "                                (df.artist == song_artist_df.name) & \\\n",
    "                                (df.length == song_artist_df.duration) & \\\n",
    "                                (df.song == song_artist_df.title), how='inner')\n",
    "    \n",
    "    songplays_table = songplays_table.select(['start_time','userId', 'level', 'sessionId', \\\n",
    "                                              'location', 'userAgent', 'artist_id','song_id'])\n",
    "    \n",
    "    # Rename songplays columns\n",
    "    songplays_table = songplays_table.withColumnRenamed(\"userId\", \"user_id\") \\\n",
    "                                     .withColumnRenamed(\"sessionId\", \"session_id\") \\\n",
    "                                     .withColumnRenamed(\"userAgent\", \"user_agent\")\n",
    "    \n",
    "    \n",
    "    # Create songplay_id column\n",
    "    songplays_table = songplays_table.withColumn(\"songplay_id\", \\\n",
    "                                                 monotonically_increasing_id())\n",
    "    \n",
    "    \n",
    "    # Select songplays columns\n",
    "    songplays_table = songplays_table.select([\"songplay_id\", \"start_time\", \"user_id\", \\\n",
    "                                              \"level\", \"song_id\", \"artist_id\", \\\n",
    "                                              \"session_id\", \"location\", \"user_agent\"])\n",
    "    \n",
    "        \n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.mode(\"overwrite\").parquet(output_data + 'songplays')\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" The module main function.\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"s3a://udacity-dend/\"\n",
    "    output_data = \"s3://spark-data-1/\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
